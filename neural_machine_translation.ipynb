{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed3f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import codecs\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from elapsedtimer import ElapsedTimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98bdc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_encoder_words\n",
    "global num_decoder_words\n",
    "global max_encoder_sequence_length\n",
    "global max_decoder_sequence_length\n",
    "global input_word_index\n",
    "global target_word_index\n",
    "global reverse_input_word_dict\n",
    "global reverse_target_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb1d0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "global path\n",
    "global num_epochs\n",
    "global batch_size\n",
    "global latent_dim\n",
    "global num_samples\n",
    "global outdir\n",
    "global verbose\n",
    "global mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dced6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_file(path, num_samples=10e13):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    input_words = set()\n",
    "    target_words = set()\n",
    "    \n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        \n",
    "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "        input_text, target_text = line.split('\\t')\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        \n",
    "        for word in input_text.split(\" \"):\n",
    "            if word not in input_words:\n",
    "                input_words.add(word)\n",
    "        \n",
    "        for word in target_text.split(\" \"):\n",
    "            if word not in target_words:\n",
    "                target_words.add(word)\n",
    "    \n",
    "    return input_texts, target_texts, input_words, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f39219f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_generation(path, num_samples, verbose=True):\n",
    "    global num_encoder_words\n",
    "    global num_decoder_words\n",
    "    global max_encoder_sequence_length\n",
    "    global max_decoder_sequence_length\n",
    "    global input_word_index\n",
    "    global target_word_index\n",
    "    global reverse_input_word_dict\n",
    "    global reverse_target_word_dict\n",
    "    input_texts, target_texts, input_words, target_words = read_input_file(path, num_samples)\n",
    "    input_words = sorted(list(input_words))\n",
    "    target_words = sorted(list(target_words))\n",
    "    num_encoder_words = len(input_words)\n",
    "    num_decoder_words = len(target_words)\n",
    "    max_encoder_sequence_length = max([len(txt.split(\" \")) for txt in input_texts])\n",
    "    max_decoder_sequence_length = max([len(txt.split(\" \")) for txt in target_texts])\n",
    "    if verbose == True:\n",
    "        print(\"Number of samples: {} \\n\".format(len(input_texts)))\n",
    "        print(\"Number of unique input tokens: {} \\n\".format(len(input_words)))\n",
    "        print(\"Number of unique output tokens: {} \\n\".format(len(output_words)))\n",
    "        print(\"Max sequence length for inputs: {} \\n\".format(max_encoder_sequence_length))\n",
    "        print(\"Max sequence length for outputs: {} \\n\".format(max_decoder_sequence_length))\n",
    "        \n",
    "    input_word_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
    "    target_word_index = dict([(word, i) for i, word in enumerate(target_words)])\n",
    "    reverse_input_word_dict = dict((i, word) for word, i in input_word_index.items())\n",
    "    reverse_target_word_dict = dict((i, word) for word, i in target_word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540cdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input_texts, target_texts=None, verbose=True):\n",
    "    global max_encoder_sequence_length\n",
    "    global num_encoder_words\n",
    "    global num_decoder_words\n",
    "    global mode\n",
    "    global input_word_index\n",
    "    global target_word_index\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_sequence_length, num_encoder_words), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(input_texts), max_decoder_sequence_length, num_decoder_words), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(input_texts), max_decoder_sequence_length, num_decoder_words), dtype='float32')\n",
    "    \n",
    "    if mode == 'train':\n",
    "        for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "            for t, word in enumerate(input_text.split(\" \")):\n",
    "                try:\n",
    "                    encoder_input_data[i, t, input_word_index[word]] = 1\n",
    "                except:\n",
    "                    print(f'word {word} encountered for the first time, skipped')\n",
    "            for t, word in enumerate(target_text.split(\" \")):\n",
    "                decoder_input_data[i, t, target_word_index[word]] = 1\n",
    "                if t > 0:\n",
    "                    try:\n",
    "                        decoder_target_data[i, t-1, target_word_index[word]] = 1\n",
    "                    except:\n",
    "                        print(f'word {word} is encountered for the first time, skipped')\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data, np.array(input_texts), np.array(target_texts)\n",
    "    else:\n",
    "        for i, input_text in enumerate(input_texts):\n",
    "            for t, word in enumerate(input_text.split(\" \")):\n",
    "                try:\n",
    "                    encoder_input_data[i, t, input_word_index[word]] = 1\n",
    "                except:\n",
    "                    print(f'word {word} is encountered for the first time, skipped')\n",
    "        return encoder_input_data, None, None, np.array(input_texts), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea2748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_enc_dec():\n",
    "    global num_encoder_words\n",
    "    global num_decoder_words\n",
    "    global latent_dim\n",
    "    global outdir\n",
    "    encoder_input = Input(shape=(None, num_encoder_words), name='encoder_input')\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder')\n",
    "    encoder_out, state_h, state_c = encoder(encoder_input)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_input = Input(shape=(None, num_decoder_words), name='decoder_input')\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_out, _, _ = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_words, activation='softmax', name='decoder_dense')\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "    print(np.shape(decoder_out))\n",
    "    \n",
    "    model = Model([encoder_input, decoder_input], decoder_out)\n",
    "    encoder_model = Model(encoder_input, encoder_states)\n",
    "    decoder_input_h = Input(shape=(latent_dim, ))\n",
    "    decoder_input_c = Input(shape=(latent_dim, ))\n",
    "    decoder_input_state = [decoder_input_h, decoder_input_c]\n",
    "    decoder_out, decoder_out_h, decoder_out_c = decoder_lstm(decoder_input, initial_state=decoder_input_state)\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "    decoder_out_state = [decoder_out_h, decoder_out_c]\n",
    "    decoder_model = Model(inputs=[decoder_input] + decoder_inp_state, output=[decoder_out] + decoder_out_state)\n",
    "    plot_model(model, show_shapes = True, to_file=outdir + 'encoder_decoder_training_model.png')\n",
    "    plot_model(encoder_model, show_shapes=True, to_file=outdir + 'encoder_model.png')\n",
    "    plot_model(decoder_model, show_shapes=True, to_file=outdir + 'decoder_model.png')\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b4faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "    global batch_size\n",
    "    global num_epochs\n",
    "    global outdir\n",
    "    print(\"Training...\", end=\"\\n\")\n",
    "    model, encoder_model, decoder_model = model_enc_dec()\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)\n",
    "    model.save(outdir+'eng_to_french_dumm.h5')\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21e28488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(num_recs, train_frac=0.8):\n",
    "    rec_indices = np.arange(num_recs)\n",
    "    np.random.shuffle(rec_indices)\n",
    "    train_count = int(num_recs*0.8)\n",
    "    train_indices = rec_indices[:train_count]\n",
    "    test_indices = rec_indices[train_count:]\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f56df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sequence, encoder_model, decoder_model):\n",
    "    global num_encoder_words\n",
    "    global num_decoder_words\n",
    "    global reverse_target_word_dict\n",
    "    global max_decoder_sequence_length\n",
    "    states_value = encoder_model.predict(input_sequence)\n",
    "    target_sequence = np.zeros((1,1,num_encoder_words))\n",
    "    target_sequence[0,0,target_word_index['\\t']] = 1\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_word, h, c = decoder_model.predict([target_sequence] + state_value)\n",
    "        sampled_word_index = np.argmax(output_word[0, -1:])\n",
    "        sampled_char = reverse_target_word_dict[sampled_word_index]\n",
    "        decoded_sentence = decoded_sentence + ' ' + sampled_char\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_sequence_length):\n",
    "            stop_condition = True\n",
    "        target_sequence = np.zeros((1,1,num_decoder_words))\n",
    "        target_sequence[0,0,sampled_word_index] = 1\n",
    "        state_value = [h, c]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
